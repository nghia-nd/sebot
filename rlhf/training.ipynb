{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install huggingface_hub transformers datasets accelerate \\\n",
    "                evaluate bitandbytes peft deepspeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_HF_PATH = 'facebook/opt-iml-max-1.3b'\n",
    "REWARD_MODEL_HF_PATH = 'facebook/opt-125m'\n",
    "\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = f'{BASE_DIR}/datasets'\n",
    "MODEL_DIR = f'{BASE_DIR}/models'\n",
    "\n",
    "MANUAL_INSTRUCTION_DATASET = f'{DATA_DIR}/manual_instructions.csv'\n",
    "NUM_WORDS = 256\n",
    "\n",
    "def split_truncate(sentence: str):\n",
    "    return \" \".join(sentence.split()[:NUM_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    PreTrainedModel\n",
    ")\n",
    "from transformers.trainer_pt_utils import nested_detach\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Union, Literal\n",
    "from dataclasses import dataclass\n",
    "from collections import deque, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_DATASET_PATH = 'ArmelR/stack-exchange-instruction'\n",
    "SFT_MODEL_PATH = f'{MODEL_DIR}/sft_opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_8_BIT_TRAINING = False\n",
    "\n",
    "if not USE_8_BIT_TRAINING:\n",
    "    sft_tokenizer = AutoTokenizer.from_pretrained(MODEL_HF_PATH)\n",
    "    sft_opt = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_HF_PATH,\n",
    "        device_map='auto',\n",
    "    )\n",
    "else:\n",
    "    sft_tokenizer = AutoTokenizer.from_pretrained(MODEL_HF_PATH)\n",
    "    sft_opt = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_HF_PATH,\n",
    "        load_in_8bit=True,\n",
    "        device_map='auto',\n",
    "    )\n",
    "    sft_opt = prepare_model_for_kbit_training(sft_opt)\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    sft_opt = get_peft_model(sft_opt, lora_config)\n",
    "    sft_opt.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = load_dataset(SFT_DATASET_PATH, split='train')\n",
    "sft_dataset = sft_dataset.remove_columns(['qid', 'date', 'metadata'])\n",
    "sft_dataset = sft_dataset.train_test_split(train_size=100000)['train']\n",
    "\n",
    "instruction_dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files=MANUAL_INSTRUCTION_DATASET,\n",
    "    split='train'\n",
    ")\n",
    "instruction_dataset = instruction_dataset.remove_columns(['rejected'])\n",
    "instruction_dataset = instruction_dataset.rename_column('chosen', 'response')\n",
    "\n",
    "final_sft_dataset = concatenate_datasets([sft_dataset, instruction_dataset])\n",
    "final_sft_dataset = final_sft_dataset.map(\n",
    "    lambda x: {\n",
    "        'inputs': (\n",
    "            f'<Question>: {split_truncate(x[\"question\"])}'\n",
    "            f'<Answer>: {split_truncate(x[\"response\"])}'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "final_sft_dataset = final_sft_dataset.map(\n",
    "    lambda x: {'input_ids': sft_tokenizer(x['inputs'])['input_ids']},\n",
    "    remove_columns=['inputs'],\n",
    "    num_proc=4\n",
    ")\n",
    "final_sft_dataset = final_sft_dataset.filter(\n",
    "    lambda x: len(x['input_ids']) <= sft_opt.config.max_position_embeddings,\n",
    "    num_proc=4\n",
    ")\n",
    "final_sft_dataset = final_sft_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sft_dataset.to_parquet(f'{DATA_DIR}/sft_dataset.parquet')\n",
    "final_sft_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=f'{DATA_DIR}/sft_dataset.parquet',\n",
    "    split='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_trainer = Trainer(\n",
    "    model=sft_opt,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        optim='paged_adamw_8bit',\n",
    "        logging_steps=1000,\n",
    "        save_strategy='no',\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=5000,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=1e-5,\n",
    "        deepspeed={\n",
    "            \"zero_optimization\": {\n",
    "                \"stage\": 2,\n",
    "                \"offload_optimizer\": {\n",
    "                    \"device\": \"cpu\",\n",
    "                    \"pin_memory\": True\n",
    "                },\n",
    "                \"allgather_partitions\": True,\n",
    "                \"allgather_bucket_size\": 5e8,\n",
    "                \"reduce_scatter\": True,\n",
    "                \"reduce_bucket_size\": 5e8,\n",
    "                \"overlap_comm\": True,\n",
    "                \"contiguous_gradients\": True\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    train_dataset = final_sft_dataset['train'],\n",
    "    eval_dataset = final_sft_dataset['test'],\n",
    "    data_collator = DataCollatorForLanguageModeling(sft_tokenizer, mlm=False),\n",
    "    tokenizer = sft_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_trainer.save_model(SFT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_DATASET_PATH = f'lvwerra/stack-exchange-paired'\n",
    "REWARD_MODEL_PATH = f'{MODEL_DIR}/sft_opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer, max_length: int, return_tensors=\"pt\"):\n",
    "      self.tokenizer = tokenizer\n",
    "      self.max_length = max_length\n",
    "      self.return_tensors = return_tensors\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        max_length = min(\n",
    "            max(\n",
    "                max(len(feature['input_ids_chosen']) for feature in features),\n",
    "                max(len(feature['input_ids_rejected']) for feature in features),\n",
    "            ),\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        features_chosen = {\n",
    "            'input_ids': [\n",
    "                feature['input_ids_chosen'][:max_length]\n",
    "                for feature in features\n",
    "            ]\n",
    "        }\n",
    "        features_rejected = {\n",
    "            'input_ids': [\n",
    "                feature['input_ids_rejected'][:max_length]\n",
    "                for feature in features\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        batch_chosen = self.tokenizer.pad(\n",
    "            features_chosen,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors=self.return_tensors,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        batch_rejected = self.tokenizer.pad(\n",
    "            features_rejected,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors=self.return_tensors,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids_chosen\": batch_chosen[\"input_ids\"],\n",
    "            \"attention_mask_chosen\": batch_chosen[\"attention_mask\"],\n",
    "            \"input_ids_rejected\": batch_rejected[\"input_ids\"],\n",
    "            \"attention_mask_rejected\": batch_rejected[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        rewards_chosen = model(\n",
    "            input_ids=inputs[\"input_ids_chosen\"],\n",
    "            attention_mask=inputs[\"attention_mask_chosen\"]\n",
    "        ).logits\n",
    "\n",
    "        rewards_rejected = model(\n",
    "            input_ids=inputs[\"input_ids_rejected\"],\n",
    "            attention_mask=inputs[\"attention_mask_rejected\"]\n",
    "        ).logits\n",
    "\n",
    "        loss = - fn.logsigmoid(rewards_chosen - rewards_rejected).mean()\n",
    "        if return_outputs:\n",
    "                return loss, {\n",
    "                    \"rewards_chosen\": rewards_chosen,\n",
    "                    \"rewards_rejected\": rewards_rejected\n",
    "                }\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        prediction_loss_only,\n",
    "        ignore_keys=None\n",
    "    ):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits_dict = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        loss = loss.detach()\n",
    "        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)\n",
    "        logits = nested_detach(logits)\n",
    "\n",
    "        # Stack accepted against rejected, mean over logits\n",
    "        # and softmax to get preferences between accepted and rejected to sum to 1\n",
    "        logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T\n",
    "\n",
    "        labels = torch.ones(logits.shape[0])\n",
    "        # labels = self._prepare_inputs(labels)\n",
    "\n",
    "        return loss, logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_HF_PATH)\n",
    "reward_opt = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_HF_PATH,\n",
    "    device_map='auto',\n",
    "    num_labels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=REWARD_DATASET_PATH,\n",
    "    split='train'\n",
    ")\n",
    "reward_dataset = reward_dataset.remove_columns(['qid', 'date', 'metadata'])\n",
    "reward_dataset = reward_dataset.rename_columns(\n",
    "    {'response_j': 'chosen', 'response_k': 'rejected'}\n",
    ")\n",
    "reward_dataset = reward_dataset.train_test_split(train_size=100000)['train']\n",
    "instruction_dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files=MANUAL_INSTRUCTION_DATASET,\n",
    "    split='train'\n",
    ")\n",
    "final_reward_dataset = concatenate_datasets([reward_dataset, instruction_dataset])\n",
    "final_reward_dataset = final_reward_dataset.map(\n",
    "    lambda x: {\n",
    "        'inputs_chosen': (\n",
    "            f'<Question>: {split_truncate(x[\"question\"])}'\n",
    "            f'<Answer>: {split_truncate(x[\"chosen\"])}'\n",
    "        ),\n",
    "        'inputs_rejected': (\n",
    "            f'<Question>: {split_truncate(x[\"question\"])}'\n",
    "            f'<Answer>: {split_truncate(x[\"rejected\"])}'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "final_reward_dataset = final_reward_dataset.map(\n",
    "    lambda x: {\n",
    "        'input_ids_chosen': reward_tokenizer(x['inputs_chosen'])['input_ids'],\n",
    "        'input_ids_rejected': reward_tokenizer(x['inputs_rejected'])['input_ids']\n",
    "    },\n",
    "    remove_columns=['inputs_chosen', 'inputs_rejected'],\n",
    "    num_proc=4\n",
    ")\n",
    "final_reward_dataset = final_reward_dataset.filter(\n",
    "    lambda x: (\n",
    "        len(x['input_ids_chosen']) <= reward_opt.config.max_position_embeddings\n",
    "        and len(x['input_ids_rejected']) <= reward_opt.config.max_position_embeddings\n",
    "    )\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reward_dataset.to_parquet(f'{DATA_DIR}/reward_dataset.parquet')\n",
    "final_reward_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=f'{DATA_DIR}/reward_dataset.parquet',\n",
    "    split='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    accuracy = np.array(preds[:,0] > preds[:,1], dtype = int).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_opt,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        gradient_checkpointing=True,\n",
    "        optim='adamw_8bit',\n",
    "        logging_steps=1000,\n",
    "        save_strategy='no',\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=1000,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=5e-5,\n",
    "        deepspeed={\n",
    "            \"zero_optimization\": {\n",
    "                \"stage\": 2,\n",
    "                \"offload_optimizer\": {\n",
    "                    \"device\": \"cpu\",\n",
    "                    \"pin_memory\": True\n",
    "                },\n",
    "                \"allgather_partitions\": True,\n",
    "                \"allgather_bucket_size\": 2e8,\n",
    "                \"reduce_scatter\": True,\n",
    "                \"reduce_bucket_size\": 2e8,\n",
    "                \"overlap_comm\": True,\n",
    "                \"contiguous_gradients\": True\n",
    "            }\n",
    "        },\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    train_dataset=final_reward_dataset['train'],\n",
    "    eval_dataset=final_reward_dataset['test'],\n",
    "    data_collator=RewardDataCollatorWithPadding(\n",
    "        reward_tokenizer, max_length=reward_opt.config.max_position_embeddings\n",
    "    ),\n",
    "    tokenizer=reward_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "reward_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_trainer.save_model(REWARD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_MODEL_PATH = f'{MODEL_DIR}/ppo_opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "ppo_opt = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    device_map='auto',\n",
    ")\n",
    "ref_opt = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "reward_opt = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_PATH,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ppo_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=f'{DATA_DIR}/reward_dataset.parquet',\n",
    "    split='train'\n",
    ").train_test_split(train_size=10000)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RLHFConfig:\n",
    "    memory_size: int = 16\n",
    "    sample_size: int = 4\n",
    "    clip_range: float = 0.2\n",
    "    value_loss_weight: float = 0.5\n",
    "    kl_div_weight: float = 0.2\n",
    "    max_length: int = 400\n",
    "\n",
    "\n",
    "class RLHFTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reward_model: PreTrainedModel,\n",
    "        reference_model: PreTrainedModel,\n",
    "        rlhf_config: RLHFConfig,\n",
    "        **regular_trainer_kwargs,\n",
    "    ):\n",
    "        super().__init__(**regular_trainer_kwargs)\n",
    "\n",
    "        assert isinstance(regular_trainer_kwargs['model'], PreTrainedModel), 'RLHFTrainer only supports HuggingFace\\'s PreTrainedModel'\n",
    "        assert self.tokenizer is not None, 'RLHFTrainer requires a tokenizer'\n",
    "        # assert self.tokenizer.padding_side == 'left', 'You should use a left padding to ensure RLHFTrainer correctness'\n",
    "        assert not getattr(self.model, 'is_encoder_decoder', False), 'RLHFTrainer only supports decoder-only architecture'\n",
    "        self.tokenizer.padding_size = 'left'\n",
    "        self.reward_model = reward_model\n",
    "        self.reference_model = reference_model\n",
    "        self.rlhf_config = rlhf_config\n",
    "        self.value_head = self._create_value_head() # Will be discarded after training\n",
    "        if self.optimizer is not None:\n",
    "            self.optimizer.add_param_group({'v_head', self.value_head})\n",
    "        else:\n",
    "            self.model.add_module('v_head', self.value_head)\n",
    "\n",
    "        self.memory = deque(maxlen=self.rlhf_config.memory_size)\n",
    "\n",
    "    def _create_value_head(self):\n",
    "        summary_dropout_prob = getattr(self.model.config, 'summary_dropout_prob', 0.1)\n",
    "        hidden_size = (\n",
    "            getattr(self.model.config, 'word_embed_proj_dim', None)\n",
    "            or getattr(self.model.config, 'hidden_size', None)\n",
    "        )\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(summary_dropout_prob),\n",
    "            nn.Linear(hidden_size, 1, bias=False),\n",
    "        ).to(self.model.device)\n",
    "\n",
    "    def _compute_batch(self, model, sentences):\n",
    "        model_outputs = model(sentences, output_hidden_states=True)\n",
    "\n",
    "        logit_indexes = sentences[:,1:]\n",
    "\n",
    "        logits = model_outputs.logits[:,:-1].float()\n",
    "        probs = torch.gather(\n",
    "            fn.softmax(logits, dim=-1), # batch, seq_len, num_embedding\n",
    "            dim=-1,\n",
    "            index=logit_indexes.unsqueeze(-1)\n",
    "        ).squeeze(dim=-1)\n",
    "\n",
    "        hidden_states = model_outputs.hidden_states[-1].float()\n",
    "        values = self.value_head(hidden_states)[:,:-1].squeeze(dim=-1)\n",
    "\n",
    "        return logits, probs, values\n",
    "\n",
    "    def _compute_loss(self, model):\n",
    "        samples = random.choices(self.memory, k=self.rlhf_config.sample_size)\n",
    "        (\n",
    "            sentences,\n",
    "            logits,\n",
    "            probs,\n",
    "            values,\n",
    "            advantages,\n",
    "            returns\n",
    "        ) = data_utils.default_collate(samples)\n",
    "\n",
    "        new_logits, new_probs, new_values = self._compute_batch(\n",
    "            model, sentences\n",
    "        )\n",
    "\n",
    "        value_losses_1 = (new_values - returns) ** 2\n",
    "        value_losses_2 = (\n",
    "            torch.clip(\n",
    "                new_values,\n",
    "                values - self.rlhf_config.clip_range,\n",
    "                values + self.rlhf_config.clip_range\n",
    "            ) - returns\n",
    "        ) ** 2\n",
    "        value_loss = torch.max(value_losses_1, value_losses_2).mean()\n",
    "\n",
    "        ratios = torch.exp(torch.log(new_probs) - torch.log(probs))\n",
    "        policy_losses_1 = - advantages * ratios\n",
    "        policy_losses_2 = - advantages * torch.clip(\n",
    "            ratios,\n",
    "            1.0 - self.rlhf_config.clip_range,\n",
    "            1.0 + self.rlhf_config.clip_range,\n",
    "        )\n",
    "        policy_loss = torch.max(policy_losses_1, policy_losses_2).mean()\n",
    "        return policy_loss + self.rlhf_config.value_loss_weight * value_loss\n",
    "\n",
    "    def _generate_sentences(self, model, prompts):\n",
    "        sentences = model.generate(\n",
    "            prompts,\n",
    "            max_length=self.rlhf_config.max_length,\n",
    "            do_sample=True,\n",
    "            return_dict_in_generate=False\n",
    "        )\n",
    "        padded_sentences = self.tokenizer.pad(\n",
    "            {'input_ids': sentences},\n",
    "            max_length=self.rlhf_config.max_length,\n",
    "            padding='max_length'\n",
    "        )['input_ids']\n",
    "        return padded_sentences.to(model.device)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        sentences = self._generate_sentences(model, inputs['input_ids'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, probs, values = self._compute_batch(\n",
    "                model, sentences\n",
    "            )\n",
    "            _, ref_probs, _ = self._compute_batch(\n",
    "                self.reference_model, sentences\n",
    "            )\n",
    "            rewards = self.reward_model(sentences).logits\n",
    "            rewards = rewards.repeat(1, logits.shape[1])\n",
    "\n",
    "            kl_div = (torch.log(probs) - torch.log(ref_probs))\n",
    "            rewards = rewards - kl_div\n",
    "\n",
    "            advantages = fn.normalize(rewards - values)\n",
    "            returns = advantages + values\n",
    "\n",
    "        for data in zip(\n",
    "            sentences, logits, probs, values, advantages, returns\n",
    "        ):\n",
    "            self.memory.append(data)\n",
    "\n",
    "        return self._compute_loss(model)\n",
    "\n",
    "\n",
    "ppo_trainer = RLHFTrainer(\n",
    "    model=ppo_opt,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        gradient_checkpointing=True,\n",
    "        optim='paged_adamw_8bit',\n",
    "        logging_steps=500,\n",
    "        save_strategy='no',\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=500,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=5e-5,\n",
    "        deepspeed={\n",
    "            \"zero_optimization\": {\n",
    "                \"stage\": 2,\n",
    "                \"offload_optimizer\": {\n",
    "                    \"device\": \"cpu\",\n",
    "                    \"pin_memory\": True\n",
    "                },\n",
    "                \"allgather_partitions\": True,\n",
    "                \"allgather_bucket_size\": 2e8,\n",
    "                \"reduce_scatter\": True,\n",
    "                \"reduce_bucket_size\": 2e8,\n",
    "                \"overlap_comm\": True,\n",
    "                \"contiguous_gradients\": True\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "    reward_model=reward_opt,\n",
    "    reference_model=ref_opt,\n",
    "    rlhf_config=RLHFConfig(\n",
    "        memory_size=16,\n",
    "        sample_size=16,\n",
    "        max_length=512\n",
    "    ),\n",
    "    tokenizer=ppo_tokenizer,\n",
    "    train_dataset=final_ppo_dataset\n",
    ")\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.save_model(PPO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPO_MODEL_PATH = f'{MODEL_DIR}/dpo_opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "dpo_opt = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    device_map='auto',\n",
    ")\n",
    "ref_opt = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=REWARD_DATASET_PATH,\n",
    "    split='train'\n",
    ")\n",
    "dpo_dataset = dpo_dataset.remove_columns(['qid', 'date', 'metadata'])\n",
    "dpo_dataset = dpo_dataset.rename_columns(\n",
    "    {'response_j': 'chosen', 'response_k': 'rejected'}\n",
    ")\n",
    "dpo_dataset = dpo_dataset.train_test_split(train_size=10000)['train']\n",
    "instruction_dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files=MANUAL_INSTRUCTION_DATASET,\n",
    "    split='train'\n",
    ")\n",
    "final_dpo_dataset = concatenate_datasets([dpo_dataset, instruction_dataset])\n",
    "final_dpo_dataset = final_dpo_dataset.map(\n",
    "    lambda x: {\n",
    "        'prompt': f'<Question>: {x[\"question\"]}<Answer>: '\n",
    "    },\n",
    "    num_proc=4,\n",
    "    remove_columns=['question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPODataCollatorWithPadding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        max_prompt_length: int = 256,\n",
    "        padding_value: int = 0,\n",
    "        label_padding_value: int = - 100, # torch cross entropy ignore index\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_prompt_length = max_prompt_length\n",
    "        self.padding_value = padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def _ignore_eos_attention_mask(self, input_ids, attention_mask):\n",
    "        eos_token_positions = set(\n",
    "            i for i, x in enumerate(input_ids)\n",
    "            if x == self.tokenizer.eos_token_id\n",
    "        )\n",
    "        new_attention_mask = [\n",
    "            0 if i in eos_token_positions else mask\n",
    "            for i, mask in enumerate(attention_mask)\n",
    "        ]\n",
    "        return new_attention_mask\n",
    "\n",
    "    def _truncate_to_length(self, sequence, max_length):\n",
    "        return {k: v[: max_length] for k,v in sequence.items()}\n",
    "\n",
    "    def _create_sequence_with_labels(self, prompt_tokens, response_tokens):\n",
    "        new_tokens = {k: prompt_tokens[k] + response_tokens[k] for k in prompt_tokens.keys()}\n",
    "        prompt_len = len(prompt_tokens['input_ids'])\n",
    "        new_tokens['labels'] = new_tokens['input_ids'].copy()\n",
    "        new_tokens['labels'][: prompt_len] = [self.label_padding_value] * prompt_len\n",
    "        return new_tokens\n",
    "\n",
    "    def tokenize_batch_element(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        chosen: str,\n",
    "        rejected: str,\n",
    "    ) -> Dict:\n",
    "        chosen_tokens = self.tokenizer(chosen, add_special_tokens=False)\n",
    "        rejected_tokens = self.tokenizer(rejected, add_special_tokens=False)\n",
    "        prompt_tokens = self.tokenizer(prompt, add_special_tokens=False)\n",
    "\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        for tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:\n",
    "            tokens['attention_mask'] = self._ignore_eos_attention_mask(\n",
    "                tokens['input_ids'], tokens['attention_mask']\n",
    "            )\n",
    "\n",
    "        longer_response_length = max(len(chosen_tokens[\"input_ids\"]), len(rejected_tokens[\"input_ids\"]))\n",
    "\n",
    "        # if combined sequence is too long, truncate the prompt\n",
    "        if len(prompt_tokens[\"input_ids\"]) + longer_response_length > self.max_length:\n",
    "            prompt_tokens = self._truncate_to_length(prompt_tokens, self.max_prompt_length)\n",
    "\n",
    "        # if that's still too long, truncate the response\n",
    "        if len(prompt_tokens[\"input_ids\"]) + longer_response_length > self.max_length:\n",
    "            max_response_length = self.max_length - self.max_prompt_length\n",
    "            chosen_tokens = self._truncate_to_length(chosen_tokens, max_response_length)\n",
    "            rejected_tokens = self._truncate_to_length(rejected_tokens, max_response_length)\n",
    "\n",
    "        chosen_sequence_tokens = self._create_sequence_with_labels(prompt_tokens, chosen_tokens)\n",
    "        rejected_sequence_tokens = self._create_sequence_with_labels(prompt_tokens, rejected_tokens)\n",
    "\n",
    "        batch = {}\n",
    "        for token_type, tokens in zip(\n",
    "            ['prompt', 'chosen', 'rejected'],\n",
    "            [prompt_tokens, chosen_sequence_tokens, rejected_sequence_tokens]\n",
    "        ):\n",
    "            for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "                if key in tokens:\n",
    "                    batch[f'{token_type}_{key}'] = tokens[key]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def collate(self, batch):\n",
    "        padded_batch = {}\n",
    "        keys = batch[0].keys()\n",
    "        for key, key_batch in zip(keys, zip(*[item.values() for item in batch])):\n",
    "            padded_batch[key] = nn.utils.rnn.pad_sequence(\n",
    "                [torch.tensor(key_item) for key_item in key_batch],\n",
    "                batch_first=True,\n",
    "                padding_value=(\n",
    "                    self.label_padding_value\n",
    "                    if key.endswith('_labels')\n",
    "                    else self.padding_value\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        tokenized_batch = []\n",
    "\n",
    "        for feature in features:\n",
    "            prompt = feature[\"prompt\"]\n",
    "            chosen = feature[\"chosen\"]\n",
    "            rejected = feature[\"rejected\"]\n",
    "\n",
    "            batch_element = self.tokenize_batch_element(prompt, chosen, rejected)\n",
    "            tokenized_batch.append(batch_element)\n",
    "\n",
    "        return self.collate(tokenized_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(tensor, length, padding_value, dim=-1):\n",
    "    padding_shape = list(tensor.shape)\n",
    "    padding_shape[dim] = length - padding_shape[dim]\n",
    "\n",
    "    if padding_shape[dim] <= 0:\n",
    "        return tensor\n",
    "\n",
    "    padding_tensor = torch.full(padding_shape, padding_value, device=tensor.device)\n",
    "    return torch.cat([tensor, padding_tensor], dim=dim)\n",
    "\n",
    "class DPOTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        ref_model: PreTrainedModel,\n",
    "        data_collator: DPODataCollatorWithPadding,\n",
    "        beta: float = 0.1,\n",
    "        disable_dropout: bool = True,\n",
    "        **regular_trainer_kwargs,\n",
    "    ):\n",
    "        if disable_dropout:\n",
    "            for module in [*model.modules(), *ref_model.modules()]:\n",
    "                if isinstance(module, torch.nn.Dropout):\n",
    "                    module.p = 0\n",
    "        super().__init__(model=model, data_collator=data_collator, **regular_trainer_kwargs)\n",
    "        self.beta = beta\n",
    "        self.ref_model = self.accelerator.prepare_model(ref_model, evaluation_mode=True)\n",
    "        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    def concatenated_inputs(self, batch: Dict[str, Union[List, torch.LongTensor]]):\n",
    "        concatenated_batch = {}\n",
    "        max_length = max(\n",
    "            batch[\"chosen_input_ids\"].shape[1],\n",
    "            batch[\"rejected_input_ids\"].shape[1]\n",
    "        )\n",
    "\n",
    "        for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "            padding_value = (\n",
    "                self.data_collator.label_padding_value\n",
    "                if key == 'labels'\n",
    "                else self.data_collator.padding_value\n",
    "            )\n",
    "            concatenated_batch[f'concatenated_{key}'] = torch.cat(\n",
    "                [\n",
    "                    pad_batch(batch[f'chosen_{key}'], max_length, padding_value),\n",
    "                    pad_batch(batch[f'rejected_{key}'], max_length, padding_value)\n",
    "                ],\n",
    "                dim=0\n",
    "            ).to(self.accelerator.device)\n",
    "\n",
    "        return concatenated_batch\n",
    "\n",
    "    def dpo_loss(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.FloatTensor,\n",
    "        policy_rejected_logps: torch.FloatTensor,\n",
    "        reference_chosen_logps: torch.FloatTensor,\n",
    "        reference_rejected_logps: torch.FloatTensor,\n",
    "    ):\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "        logits = pi_logratios - ref_logratios\n",
    "\n",
    "        losses = - fn.logsigmoid(self.beta * logits)\n",
    "        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "\n",
    "        return losses, chosen_rewards, rejected_rewards\n",
    "\n",
    "    def _get_batch_logps(\n",
    "        self,\n",
    "        logits: torch.FloatTensor,\n",
    "        labels: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        labels = labels[:, 1:].clone()\n",
    "        logits = logits[:, :-1, :]\n",
    "        loss_mask = labels != self.data_collator.label_padding_value\n",
    "\n",
    "        # dummy token; we'll ignore the losses on these tokens later\n",
    "        labels[labels == self.data_collator.label_padding_value] = 0\n",
    "\n",
    "        per_token_logps = torch.gather(\n",
    "            logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)\n",
    "        ).squeeze(dim=2)\n",
    "\n",
    "        return (per_token_logps * loss_mask).sum(dim=-1)\n",
    "\n",
    "    def concatenated_forward(\n",
    "        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n",
    "    ):\n",
    "        concatenated_batch = self.concatenated_inputs(batch)\n",
    "        len_chosen = len(batch[\"chosen_labels\"])\n",
    "\n",
    "        all_logits = model(\n",
    "            concatenated_batch[\"concatenated_input_ids\"],\n",
    "            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "        ).logits.to(torch.float32)\n",
    "\n",
    "        all_logps = self._get_batch_logps(\n",
    "            all_logits,\n",
    "            concatenated_batch[\"concatenated_labels\"],\n",
    "        )\n",
    "\n",
    "        chosen_logps = all_logps[:len_chosen]\n",
    "        rejected_logps = all_logps[len_chosen:]\n",
    "\n",
    "        chosen_logits = all_logits[:len_chosen]\n",
    "        rejected_logits = all_logits[len_chosen:]\n",
    "\n",
    "        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)\n",
    "\n",
    "    def get_batch_metrics(\n",
    "        self,\n",
    "        model,\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n",
    "    ):\n",
    "        metrics = {}\n",
    "\n",
    "        (\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            policy_chosen_logits,\n",
    "            policy_rejected_logits,\n",
    "        ) = self.concatenated_forward(model, batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                reference_chosen_logps,\n",
    "                reference_rejected_logps,\n",
    "                _,\n",
    "                _,\n",
    "            ) = self.concatenated_forward(self.ref_model, batch)\n",
    "\n",
    "        losses, chosen_rewards, rejected_rewards = self.dpo_loss(\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            reference_chosen_logps,\n",
    "            reference_rejected_logps,\n",
    "        )\n",
    "        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n",
    "\n",
    "        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n",
    "        metrics[f\"{prefix}rewards/chosen\"] = chosen_rewards.cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}rewards/rejected\"] = rejected_rewards.cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}rewards/margins\"] = (chosen_rewards - rejected_rewards).cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logps/rejected\"] = policy_rejected_logps.detach().cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logps/chosen\"] = policy_chosen_logps.detach().cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logits/rejected\"] = policy_rejected_logits.detach().cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logits/chosen\"] = policy_chosen_logits.detach().cpu().numpy().mean()\n",
    "\n",
    "        return losses.mean(), metrics\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module],\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        return_outputs=False,\n",
    "    ):\n",
    "        loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"train\")\n",
    "\n",
    "        # force log the metrics\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.store_metrics(metrics, train_eval=\"train\")\n",
    "\n",
    "        if return_outputs:\n",
    "            return (loss, metrics)\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module],\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(model, \"config\"):\n",
    "                ignore_keys = getattr(model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"eval\")\n",
    "\n",
    "        # force log the metrics\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.store_metrics(metrics, train_eval=\"eval\")\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss.detach(), None, None)\n",
    "\n",
    "        # logits for the chosen and rejected samples from model\n",
    "        logits_dict = {\n",
    "            \"eval_logits/chosen\": metrics[\"eval_logits/chosen\"],\n",
    "            \"eval_logits/rejected\": metrics[\"eval_logits/rejected\"],\n",
    "        }\n",
    "\n",
    "        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)\n",
    "        logits = torch.stack(logits).mean(axis=1)\n",
    "        labels = torch.zeros(logits.shape[0])\n",
    "\n",
    "        return (loss.detach(), logits, labels)\n",
    "\n",
    "    def store_metrics(self, metrics: Dict[str, float], train_eval: Literal[\"train\", \"eval\"] = \"train\") -> None:\n",
    "        for key, value in metrics.items():\n",
    "            self._stored_metrics[train_eval][key].append(value)\n",
    "\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        # logs either has 'loss' or 'eval_loss'\n",
    "        train_eval = \"train\" if \"loss\" in logs else \"eval\"\n",
    "        # Add averaged stored metrics to logs\n",
    "        for key, metrics in self._stored_metrics[train_eval].items():\n",
    "            logs[key] = torch.tensor(metrics).mean().item()\n",
    "        del self._stored_metrics[train_eval]\n",
    "        return super().log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_opt,\n",
    "    ref_model=ref_opt,\n",
    "    beta=0.1,\n",
    "    train_dataset=final_dpo_dataset,\n",
    "    tokenizer=dpo_tokenizer,\n",
    "    data_collator=DPODataCollatorWithPadding(\n",
    "        tokenizer=dpo_tokenizer,\n",
    "        max_length=200,\n",
    "        max_prompt_length=50,\n",
    "        padding_value=dpo_tokenizer.pad_token_id,\n",
    "        label_padding_value=-100,\n",
    "    ),\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=True,\n",
    "        optim='paged_adamw_8bit',\n",
    "        logging_steps=50,\n",
    "        save_strategy='no',\n",
    "        evaluation_strategy='no',\n",
    "        num_train_epochs=1,\n",
    "        remove_unused_columns=False,\n",
    "        deepspeed={\n",
    "            \"zero_optimization\": {\n",
    "                \"stage\": 2,\n",
    "                \"offload_optimizer\": {\n",
    "                    \"device\": \"cpu\",\n",
    "                    \"pin_memory\": True\n",
    "                },\n",
    "                \"allgather_partitions\": True,\n",
    "                \"allgather_bucket_size\": 2e8,\n",
    "                \"reduce_scatter\": True,\n",
    "                \"reduce_bucket_size\": 2e8,\n",
    "                \"overlap_comm\": True,\n",
    "                \"contiguous_gradients\": True\n",
    "            }\n",
    "        },\n",
    "    ),\n",
    "\n",
    ")\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model(DPO_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
